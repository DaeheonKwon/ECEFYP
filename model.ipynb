{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Set the random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "'''\n",
    "assume that datasets are given, after preprocessing\n",
    "'''\n",
    "# Set the random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create a random dataset\n",
    "dataset_size = 1200\n",
    "data = torch.randn(dataset_size, 22, 128, 8)\n",
    "\n",
    "labels = torch.randint(2, (dataset_size,))\n",
    "train_dataset = list(zip(data[:500], torch.zeros(500, dtype=torch.int)))\n",
    "train_dataset += list(zip(data[500:1000], torch.ones(500, dtype=torch.int)))\n",
    "\n",
    "test_dataset = list(zip(data[1000:1100], torch.zeros(200, dtype=torch.int)))\n",
    "test_dataset += list(zip(data[1100:], torch.ones(200, dtype=torch.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch starting.\n",
      "Epoch [1] Loss: 1.7253\n",
      "1th epoch starting.\n",
      "Epoch [2] Loss: 1.4242\n",
      "2th epoch starting.\n",
      "Epoch [3] Loss: 2.5249\n",
      "3th epoch starting.\n",
      "Epoch [4] Loss: 2.0599\n",
      "4th epoch starting.\n",
      "Epoch [5] Loss: 2.1095\n",
      "5th epoch starting.\n",
      "Epoch [6] Loss: 2.0052\n",
      "6th epoch starting.\n",
      "Epoch [7] Loss: 1.9216\n",
      "7th epoch starting.\n",
      "Epoch [8] Loss: 1.8510\n",
      "8th epoch starting.\n",
      "Epoch [9] Loss: 1.8845\n",
      "9th epoch starting.\n",
      "Epoch [10] Loss: 1.9712\n",
      "10th epoch starting.\n",
      "Epoch [11] Loss: 1.7686\n",
      "11th epoch starting.\n",
      "Epoch [12] Loss: 1.5321\n",
      "12th epoch starting.\n",
      "Epoch [13] Loss: 1.4954\n",
      "13th epoch starting.\n",
      "Epoch [14] Loss: 1.3076\n",
      "14th epoch starting.\n",
      "Epoch [15] Loss: 1.3138\n",
      "15th epoch starting.\n",
      "Epoch [16] Loss: 1.0504\n",
      "16th epoch starting.\n",
      "Epoch [17] Loss: 1.5882\n",
      "17th epoch starting.\n",
      "Epoch [18] Loss: 1.7432\n",
      "18th epoch starting.\n",
      "Epoch [19] Loss: 0.7350\n",
      "19th epoch starting.\n",
      "Epoch [20] Loss: 0.7027\n",
      "20th epoch starting.\n",
      "Epoch [21] Loss: 1.6081\n",
      "21th epoch starting.\n",
      "Epoch [22] Loss: 1.4896\n",
      "22th epoch starting.\n",
      "Epoch [23] Loss: 1.4938\n",
      "23th epoch starting.\n",
      "Epoch [24] Loss: 1.4880\n",
      "24th epoch starting.\n",
      "Epoch [25] Loss: 1.4891\n",
      "25th epoch starting.\n",
      "Epoch [26] Loss: 1.4866\n",
      "26th epoch starting.\n",
      "Epoch [27] Loss: 1.4884\n",
      "27th epoch starting.\n",
      "Epoch [28] Loss: 1.4897\n",
      "28th epoch starting.\n",
      "Epoch [29] Loss: 1.4899\n",
      "29th epoch starting.\n",
      "Epoch [30] Loss: 1.4904\n",
      "30th epoch starting.\n",
      "Epoch [31] Loss: 1.4901\n",
      "31th epoch starting.\n",
      "Epoch [32] Loss: 1.4904\n",
      "32th epoch starting.\n",
      "Epoch [33] Loss: 1.4899\n",
      "33th epoch starting.\n",
      "Epoch [34] Loss: 1.4909\n",
      "34th epoch starting.\n",
      "Epoch [35] Loss: 1.4902\n",
      "35th epoch starting.\n",
      "Epoch [36] Loss: 1.4912\n",
      "36th epoch starting.\n",
      "Epoch [37] Loss: 1.4913\n",
      "37th epoch starting.\n",
      "Epoch [38] Loss: 1.4909\n",
      "38th epoch starting.\n",
      "Epoch [39] Loss: 1.4916\n",
      "39th epoch starting.\n",
      "Epoch [40] Loss: 1.4931\n",
      "40th epoch starting.\n",
      "Epoch [41] Loss: 1.4919\n",
      "41th epoch starting.\n",
      "Epoch [42] Loss: 1.4923\n",
      "42th epoch starting.\n",
      "Epoch [43] Loss: 1.4929\n",
      "43th epoch starting.\n",
      "Epoch [44] Loss: 1.4926\n",
      "44th epoch starting.\n",
      "Epoch [45] Loss: 1.4942\n",
      "45th epoch starting.\n",
      "Epoch [46] Loss: 1.4934\n",
      "46th epoch starting.\n",
      "Epoch [47] Loss: 1.4937\n",
      "47th epoch starting.\n",
      "Epoch [48] Loss: 1.4940\n",
      "48th epoch starting.\n",
      "Epoch [49] Loss: 1.4942\n",
      "49th epoch starting.\n",
      "Epoch [50] Loss: 1.4945\n",
      "50th epoch starting.\n",
      "Epoch [51] Loss: 1.4951\n",
      "51th epoch starting.\n",
      "Epoch [52] Loss: 1.4949\n",
      "52th epoch starting.\n",
      "Epoch [53] Loss: 1.4962\n",
      "53th epoch starting.\n",
      "Epoch [54] Loss: 1.4965\n",
      "54th epoch starting.\n",
      "Epoch [55] Loss: 1.4974\n",
      "55th epoch starting.\n",
      "Epoch [56] Loss: 1.4968\n",
      "56th epoch starting.\n",
      "Epoch [57] Loss: 1.4979\n",
      "57th epoch starting.\n",
      "Epoch [58] Loss: 1.4972\n",
      "58th epoch starting.\n",
      "Epoch [59] Loss: 1.4977\n",
      "59th epoch starting.\n",
      "Epoch [60] Loss: 1.4992\n",
      "60th epoch starting.\n",
      "Epoch [61] Loss: 1.4993\n",
      "61th epoch starting.\n",
      "Epoch [62] Loss: 1.5017\n",
      "62th epoch starting.\n",
      "Epoch [63] Loss: 1.5047\n",
      "63th epoch starting.\n",
      "Epoch [64] Loss: 1.5088\n",
      "64th epoch starting.\n",
      "Epoch [65] Loss: 1.5021\n",
      "65th epoch starting.\n",
      "Epoch [66] Loss: 1.5089\n",
      "66th epoch starting.\n",
      "Epoch [67] Loss: 1.5103\n",
      "67th epoch starting.\n",
      "Epoch [68] Loss: 1.5059\n",
      "68th epoch starting.\n",
      "Epoch [69] Loss: 1.5182\n",
      "69th epoch starting.\n",
      "Epoch [70] Loss: 1.5045\n",
      "70th epoch starting.\n",
      "Epoch [71] Loss: 1.5082\n",
      "71th epoch starting.\n",
      "Epoch [72] Loss: 1.5186\n",
      "72th epoch starting.\n",
      "Epoch [73] Loss: 1.5062\n",
      "73th epoch starting.\n",
      "Epoch [74] Loss: 1.5343\n",
      "74th epoch starting.\n",
      "Epoch [75] Loss: 1.5266\n",
      "75th epoch starting.\n",
      "Epoch [76] Loss: 1.5489\n",
      "76th epoch starting.\n",
      "Epoch [77] Loss: 1.5378\n",
      "77th epoch starting.\n",
      "Epoch [78] Loss: 1.5621\n",
      "78th epoch starting.\n",
      "Epoch [79] Loss: 1.5437\n",
      "79th epoch starting.\n",
      "Epoch [80] Loss: 1.5411\n",
      "80th epoch starting.\n",
      "Epoch [81] Loss: 1.5572\n",
      "81th epoch starting.\n",
      "Epoch [82] Loss: 1.8333\n",
      "82th epoch starting.\n",
      "Epoch [83] Loss: 1.4300\n",
      "83th epoch starting.\n",
      "Epoch [84] Loss: 1.4792\n",
      "84th epoch starting.\n",
      "Epoch [85] Loss: 1.4799\n",
      "85th epoch starting.\n",
      "Epoch [86] Loss: 1.4177\n",
      "86th epoch starting.\n",
      "Epoch [87] Loss: 1.1478\n",
      "87th epoch starting.\n",
      "Epoch [88] Loss: 1.8101\n",
      "88th epoch starting.\n",
      "Epoch [89] Loss: 1.7686\n",
      "89th epoch starting.\n",
      "Epoch [90] Loss: 1.7386\n",
      "90th epoch starting.\n",
      "Epoch [91] Loss: 1.7395\n",
      "91th epoch starting.\n",
      "Epoch [92] Loss: 1.7396\n",
      "92th epoch starting.\n",
      "Epoch [93] Loss: 1.7374\n",
      "93th epoch starting.\n",
      "Epoch [94] Loss: 1.7370\n",
      "94th epoch starting.\n",
      "Epoch [95] Loss: 1.5378\n",
      "95th epoch starting.\n",
      "Epoch [96] Loss: 2.7593\n",
      "96th epoch starting.\n",
      "Epoch [97] Loss: 2.4402\n",
      "97th epoch starting.\n",
      "Epoch [98] Loss: 1.8222\n",
      "98th epoch starting.\n",
      "Epoch [99] Loss: 2.0943\n",
      "99th epoch starting.\n",
      "Epoch [100] Loss: 1.7281\n",
      "Time ellapsed in training is: 20.545417070388794\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "npc_loss() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    113\u001b[0m output \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m--> 114\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    116\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    117\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(labels\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: npc_loss() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "class SciCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SciCNN, self).__init__()        \n",
    "\n",
    "        self.inception1 = Inception(8, 8, 16, 8, 8)\n",
    "        self.maxpool1 = nn.MaxPool2d((1, 4), stride=(1, 4), ceil_mode=True)\n",
    "        self.inception2 = Inception(16, 16, 8, 16, 4)\n",
    "        self.maxpool2 = nn.MaxPool2d((1, 4), stride=(1, 4), ceil_mode=True)\n",
    "        self.inception3 = Inception(32, 32, 4, 32, 2)\n",
    "        self.maxpool3 = nn.MaxPool2d((1, 8), stride=(1, 8), ceil_mode=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(22*64, 16)\n",
    "        self.npc = NPC()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.inception1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.inception2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.inception3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1, ch1_kernel, ch2, ch2_kernel):\n",
    "        super(Inception, self).__init__()\n",
    "        self.branch1 = BasicConv1d(in_channels, ch1, kernel=(1, ch1_kernel), padding=(0, (ch1_kernel-1)//2))\n",
    "        \n",
    "        self.branch2 = BasicConv1d(in_channels, ch2, kernel=(1, ch2_kernel), padding=(0, (ch2_kernel-1)//2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        return torch.cat([branch1, branch2], dim=1)\n",
    "\n",
    "class BasicConv1d(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel, padding):\n",
    "        super(BasicConv1d, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=kernel, padding=padding, bias=False),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class NPC(nn.Module):\n",
    "    def __init__(self, num_clusters=256):\n",
    "        super(NPC, self).__init__()\n",
    "        # 256 predefined positions of NPC clusters\n",
    "        self.position = nn.Parameter(torch.randn(num_clusters, 16, 1))\n",
    "        self.label = nn.Parameter(torch.randint(1, (num_clusters,)), requires_grad=False)\n",
    "\n",
    "model = SciCNN().to(device)\n",
    "\n",
    "def npc_loss(output, label, model):\n",
    "    # output: (batch    _size, 16, 1)\n",
    "    mean_output = torch.mean(output, dim=0)\n",
    "    distances = torch.norm(mean_output - model.npc.position)\n",
    "    closest_position_index = torch.argmin(distances)\n",
    "    closest_position = model.npc.position[closest_position_index]\n",
    "    model.npc.label[closest_position_index] = torch.max(label)\n",
    "    loss = torch.norm(mean_output - closest_position)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "model = SciCNN().to(device)\n",
    "loss_function = npc_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(100) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader) :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels, model)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print (\"Epoch [{}] Loss: {:.4f}\".format(epoch+1, train_loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "with torch.no_grad():  #using context manager\n",
    "    for images, labels in test_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "        test_loss += loss_function(output, labels).item()\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
